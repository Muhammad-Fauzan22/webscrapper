name: Autonomous Scraper Swarm
on:
  schedule:
    - cron: "*/15 * * * *"  # Every 15 minutes
  workflow_dispatch:  # Manual trigger

jobs:
  swarm-execution:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        shard: [1, 2, 3, 4]  # Parallel execution shards
        
    env:
      PYTHONUNBUFFERED: 1
      DEBUG_MODE: 0
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          
      - name: Configure credentials
        env:
          GOOGLE_CREDS_JSON: ${{ secrets.GOOGLE_CREDS }}
          MONGO_CONN_STR: ${{ secrets.MONGO_CONN_STR }}
          DEEPSEEK_KEY: ${{ secrets.DEEPSEEK_KEY }}
          CLAUDE_KEY: ${{ secrets.CLAUDE_KEY }}
          PERPLEXITY_KEY: ${{ secrets.PERPLEXITY_KEY }}
          CYPHER_KEY: ${{ secrets.CYPHER_KEY }}
          GEMMA_KEY: ${{ secrets.GEMMA_KEY }}
        run: |
          echo "$GOOGLE_CREDS_JSON" > service_account.json
          echo "MONGO_CONN_STR=$MONGO_CONN_STR" >> $GITHUB_ENV
          
      - name: Run AI Swarm
        run: |
          python -m swarm.core --shard ${{ matrix.shard }}
        
      - name: Upload logs on failure
        if: ${{ failure() }}
        uses: actions/upload-artifact@v3
        with:
          name: swarm-logs-${{ matrix.shard }}
          path: |
            *.log
            logs/
